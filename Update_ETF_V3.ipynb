{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update ETF File\n",
    "\n",
    "@2020/07/21 By Chunlei Zhou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl as xl\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sheet_property(sheet):\n",
    "    '''\n",
    "    This function extract properties that will be used later on in the updating process.\n",
    "    Input:  sheet: a worksheet that will be updated\n",
    "    Output: nrow: how many rows are there in the worksheet before update\n",
    "            ncol: how many columns are there in the worksheet before update\n",
    "            starts: the date we will update the sheet from\n",
    "    '''\n",
    "    nrow = sheet.max_row\n",
    "    ncol = sheet.max_column\n",
    "    start_datetime = sheet.cell(row = nrow,column=1).value\n",
    "    yyyy = start_datetime.strftime(\"%Y\")\n",
    "    mm = start_datetime.strftime(\"%m\")\n",
    "    dd = start_datetime.strftime(\"%d\")\n",
    "    qstart = yyyy + '-' + mm + '-' + dd\n",
    "    spstart = mm + '-'+ dd + '-' + yyyy\n",
    "    starts = {'sp500': spstart, 'quotes': qstart}\n",
    "    return nrow, ncol, starts\n",
    "\n",
    "def browser_quotes(executable_path):\n",
    "    '''\n",
    "    This function prepares the firefox browser\n",
    "    '''\n",
    "    browser = webdriver.Firefox(executable_path = executable_path)\n",
    "    return browser\n",
    "\n",
    "def adsblocker_Chrome(path_to_extension):\n",
    "    '''\n",
    "    This function builds the Chrome driver with the Ads Blocker (as an extension).\n",
    "    If you do not want to block the adds, then do not call this fucntion, code your own driver instead.\n",
    "    '''\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument('load-extension=' + path_to_extension)\n",
    "    browser = webdriver.Chrome(ChromeDriverManager().install(), chrome_options=chrome_options)\n",
    "    return browser\n",
    "\n",
    "def extract_sp_links(sheet):\n",
    "    '''\n",
    "    This function extracts all links from sheet %S&P500 and stores them in a dictionary\n",
    "    '''\n",
    "    splinks = {}\n",
    "    for val in sheet.iter_cols(min_row = 5, min_col = 12, max_col = 20, max_row = 6, values_only = True):\n",
    "        link = val[0]\n",
    "        name = val[1]\n",
    "        if name not in splinks.keys():\n",
    "            splinks[name] = link\n",
    "    return splinks\n",
    "\n",
    "def extract_quotes_hist_link(key):\n",
    "    '''\n",
    "    This fucntion extract all links from yahoo finance for index in the sheet %Qoutes\n",
    "    Input: key: the name of the ETF\n",
    "    Output: link: direct to historical data page of the ETF\n",
    "    '''\n",
    "    print('Extracting', key, 'Information from website...')\n",
    "    \n",
    "    # Search the key using search box\n",
    "    # Find location of the search box and put key into the box\n",
    "    browser.find_element_by_xpath('//*[@id=\"yfin-usr-qry\"]').send_keys(key)\n",
    "    time.sleep(10)\n",
    "    # Click search\n",
    "    browser.find_element_by_xpath('//*[@id=\"header-desktop-search-button\"]').click()\n",
    "    \n",
    "    # Extract Historical Data Link\n",
    "    time.sleep(5)\n",
    "    li = browser.find_element(By.XPATH, '//li[@data-test = \"HISTORICAL_DATA\"]')\n",
    "    hist = li.find_element_by_xpath('.//a')\n",
    "    link = hist.get_attribute('href')\n",
    "    \n",
    "    return link\n",
    "\n",
    "def locate_quotes_table(link, start, end):\n",
    "    '''\n",
    "    This fucntion locate the table contains quotes data we want on the webpage.\n",
    "    Input:  link: direct to historical data page of the ETF we are updating\n",
    "            start: the date from which we update the ETF\n",
    "            end: the date till which we update the ETF\n",
    "    Output: tbody: a list contains all rows of the table we located\n",
    "            thead: a list contains names of all columns of the table we located\n",
    "    '''\n",
    "    print('Locating target table on website...')\n",
    "    # Open the web page\n",
    "    browser.get(link)\n",
    "    \n",
    "    # Scroll web page to a suitable location\n",
    "    selector = browser.find_element(By.XPATH, '//div[@class = \"Pt(15px) drop-down-selector historical\"]')\n",
    "    apply = selector.find_element_by_xpath('.//button')\n",
    "    apply.send_keys(Keys.DOWN)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Find the date manipulator\n",
    "    date = selector.find_elements(By.XPATH, '//div[@data-test = \"dropdown\"]')\n",
    "    date[2].click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Set the start_date\n",
    "    browser.find_element(By.XPATH, '//input[@name = \"startDate\"]').send_keys(start)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Set the end_date\n",
    "    browser.find_element(By.XPATH, '//input[@name = \"endDate\"]').send_keys(end)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Click \"Done\" to submit all the settings\n",
    "    date_menu = browser.find_element(By.XPATH, '//div[@data-test = \"date-picker-menu\"]')\n",
    "    buttons = date_menu.find_elements_by_xpath('.//button')\n",
    "    buttons[-2].click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Find the dropdown menu to set what data to display:\n",
    "    container = browser.find_elements(By.XPATH, '//div[@data-test = \"select-container\"]')\n",
    "    container[0].click()\n",
    "    time.sleep(2)\n",
    "    # Show Historical Prices\n",
    "    hist_menu = browser.find_element(By.XPATH, '//div[@data-test = \"historicalFilter-menu\"]')\n",
    "    hist_menu.find_element(By.XPATH, '//div[@data-value = \"history\"]').click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Find the dropdonw menu to set the data frequency\n",
    "    container[1].click()\n",
    "    freq_menu = browser.find_element(By.XPATH, '//div[@data-test = \"historicalFrequency-menu\"]')\n",
    "    time.sleep(2)\n",
    "    # Set Frequency as 'Daily'\n",
    "    freq_menu.find_element(By.XPATH, '//div[@data-value = \"1d\"]').click()\n",
    "    \n",
    "    # Click 'Apply' button to submit all the settings\n",
    "    apply.click()\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Locate table\n",
    "    table = browser.find_element(By.XPATH, '//table[@data-test = \"historical-prices\"]')\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Scroll the web page to bottom\n",
    "    html = browser.find_element_by_tag_name('html')\n",
    "    html.send_keys(Keys.END)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Extract all rows into a list\n",
    "    tbody = table.find_element_by_xpath('.//tbody')\n",
    "    # Extract all heads into a list\n",
    "    thead = table.find_element_by_xpath('.//thead')\n",
    "    time.sleep(5)\n",
    "    \n",
    "    return tbody, thead\n",
    "\n",
    "def locate_sp_table(key, link, spstart, spend):\n",
    "    '''\n",
    "    This function locates the table contains the S&P500 data on the web page\n",
    "    Input:  key: the name of ETF\n",
    "            link: direct to the historical data page of the ETF we are updating\n",
    "            spstart: the date from when we are updating the ETF\n",
    "            spend: the end date till when we are updating the ETF\n",
    "    Output: tbody: a list contains all rows of the table we located\n",
    "            thead: a list contains names of all columns of the table we located\n",
    "    '''\n",
    "    print('Extracting', key, 'Information from website...')\n",
    "    \n",
    "    # Load webpage\n",
    "    browser.get(link)\n",
    "    time.sleep(10)\n",
    "    \n",
    "    # Find the dropdown menu to select Historical Data for display\n",
    "    time_frame = browser.find_element(By.XPATH,'//select[@id = \"data_interval\"]')\n",
    "    time.sleep(2)\n",
    "    # Scroll the page to a suitable position\n",
    "    time_frame.send_keys(Keys.DOWN)\n",
    "    time.sleep(2)\n",
    "    # Select historical data\n",
    "    browser.find_element(By.XPATH, './/select[@id = \"data_interval\"]/option[1]')\n",
    "    time.sleep(2)\n",
    "    # click and submit the selection\n",
    "    time_frame.click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Find data manipulator and set the updating data range\n",
    "    browser.find_element(By.XPATH,'//div[@id = \"widgetField\"]').click()\n",
    "    time.sleep(2)\n",
    "    date_menu = browser.find_element(By.XPATH,'//div[@id = \"ui-datepicker-div\"]')\n",
    "    time.sleep(2)\n",
    "    # Set start date\n",
    "    start_bar = date_menu.find_element(By.XPATH, '//input[@id = \"startDate\"]')\n",
    "    start_bar.clear()\n",
    "    time.sleep(2)\n",
    "    start_bar.send_keys(spstart)\n",
    "    time.sleep(2)\n",
    "    # Set end date\n",
    "    end_bar = date_menu.find_element(By.XPATH, '//input[@id = \"endDate\"]')\n",
    "    end_bar.clear()\n",
    "    time.sleep(2)\n",
    "    end_bar.send_keys(spend)\n",
    "    time.sleep(2)\n",
    "    # Click 'Apply' button to submit all the settings\n",
    "    apply = date_menu.find_element(By.XPATH, '//a[@id = \"applyBtn\"]')\n",
    "    apply.click()\n",
    "    \n",
    "    # Locate table\n",
    "    time.sleep(5)\n",
    "    table = browser.find_element(By.XPATH,'//table[@id = \"curr_table\"]')\n",
    "    \n",
    "    # Scroll the web page to bottom\n",
    "    html = browser.find_element_by_tag_name('html')\n",
    "    html.send_keys(Keys.END)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Extract all heads from table to a list\n",
    "    spthead = table.find_element_by_xpath('.//thead')\n",
    "    # Extract all rows from table to a list\n",
    "    sptbody = table.find_element_by_xpath('.//tbody')\n",
    "    \n",
    "    return sptbody, spthead\n",
    "\n",
    "def get_index(thead, target, Date):\n",
    "    '''\n",
    "    This function finds the index of the data we use to update the sheet\n",
    "    Input:  thead: a xpath of the table head\n",
    "            target: the name of the column under which is the data we need\n",
    "            Date: the name of the date column in the web page table\n",
    "    Output: target_index: the index of the table column under which is the data we need\n",
    "            date_indx: the index of the date column\n",
    "    '''\n",
    "    ths = thead.find_elements_by_xpath('.//th')\n",
    "    heads = []\n",
    "    # Extract all heads as strings from the table head row\n",
    "    for th in ths:\n",
    "        heads.append(th.text)\n",
    "    # Obtain the index of date column\n",
    "    date_index = heads.index(Date)\n",
    "    # Obtain the index of column under which is the data we need\n",
    "    target_index = heads.index(target)\n",
    "    return target_index, date_index\n",
    "    \n",
    "def interpret_table(tbody, data_index, date_index):\n",
    "    '''\n",
    "    This function extract only the date data and the data we need from the whole table on web page\n",
    "    Input:  tbody: a xpath of the table body\n",
    "            data_index: the index of column under which is the data we need\n",
    "            date_index: the index of the date column\n",
    "    Output: asc_data[1:]: a list stores all the data we need to update the sheet from the next day of the start date till the end date\n",
    "            asc_date[1:]: a list stores all the date corresponding to the date range we set\n",
    "    '''\n",
    "    print('Scraping information from target table...')\n",
    "    \n",
    "    # Extract all rows from table body\n",
    "    trs = tbody.find_elements_by_xpath('.//tr')\n",
    "    date = []\n",
    "    data = []\n",
    "    for tr in trs:\n",
    "        tds = tr.find_elements_by_xpath('.//td')\n",
    "        if data_index <= len(tds) - 1:\n",
    "            # Find the data we need using index\n",
    "            data.append(tds[data_index].text)\n",
    "            # Find the corresponding date using index\n",
    "            date.append(tds[date_index].text)\n",
    "    # On the web page the data is from end to start, we want it to from start to end. Re-order data\n",
    "    asc_data = data[::-1]\n",
    "    ase_date = date[::-1]\n",
    "    \n",
    "    # The first data/date is the last row in the sheet. We only need data/date after it. Remove duplicate information\n",
    "    return asc_data[1:], ase_date[1:]\n",
    "\n",
    "\n",
    "def construct_dataframe(dic):\n",
    "    '''\n",
    "    This function transform the dictionary stores all information scraped from website to a data frame. \n",
    "    We need a data frame to further write the sheet we are updating.\n",
    "    '''\n",
    "    df = pd.DataFrame(dic)\n",
    "    \n",
    "    # Turn the data format of date to the one we want\n",
    "    df['Date']= pd.to_datetime(df['Date'])\n",
    "    df['Date'] = df['Date'].dt.date\n",
    "    \n",
    "    # Turn all string data to float data\n",
    "    for col in df.columns[1:]:\n",
    "        try:\n",
    "            df[col] = pd.to_numeric(df[col])\n",
    "        except:\n",
    "            print('Cannot convert value in', col, 'to float. Save as string.')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def update_quotes_sheet(sheet, df):\n",
    "    '''\n",
    "    This fucntion write the sheet %Quotes using a data frame.\n",
    "    '''\n",
    "    for row in range(len(df)):\n",
    "        sheet.append(df.iloc[row].tolist())\n",
    "\n",
    "def update_sp500_sheet(sheet, df, nrow, L = 12, A = 1):\n",
    "    '''\n",
    "    This function write the sheet %S&P500 using a data frame.\n",
    "    '''\n",
    "    # Update columns L to T\n",
    "    data = df.iloc[:,1:]\n",
    "    for row in range(len(data)):\n",
    "        values = data.iloc[row].tolist()\n",
    "        for col, entry in enumerate(values, start = L):\n",
    "            sheet.cell(row = nrow + row, column = col, value = entry)\n",
    "\n",
    "    # Update column A\n",
    "    dates = list(df.iloc[:,0])\n",
    "    for row, entry in enumerate(dates, start = A):\n",
    "        sheet.cell(row = nrow + row, column = A, value = entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All quotes data are from yahoo finance\n",
    "yahoo = 'https://finance.yahoo.com/' # Link of yahoo finance\n",
    "\n",
    "#TODO: Have to change these paths to your own before you can run the code!\n",
    "executable_path = '/Users/zhouchunlei/Downloads/geckodriver' # path of my firefox\n",
    "path_to_extension = r'/Users/zhouchunlei/Desktop/JOB HUNTING/Internship/Week 1 7.17-7.23/4.15.0_0' # path of my Chrome Adblocker extension\n",
    "\n",
    "# TODO: Have to change the file path to your own directory before you can run the code!\n",
    "file = xl.load_workbook('01 ETF Portfolio-updated.xlsx') # Load the workbook\n",
    "# Obtain the worksheets\n",
    "sp500 = file['%S&P 500']\n",
    "quotes = file['%QUOTES']\n",
    "\n",
    "# Obtain properties for both sheets\n",
    "# SP500\n",
    "spnrow, spncol, spstarts = get_sheet_property(sheet = sp500)\n",
    "spstart = spstarts['sp500']\n",
    "# Quotes\n",
    "qnrow, qncol, qstarts = get_sheet_property(sheet = quotes)\n",
    "qstart = qstarts['quotes']\n",
    "\n",
    "# TODO: Set your own end date before you run the code but do NOT change the format\n",
    "qend = '2020-07-21' # Format for quotes end date: yyyy-mm-dd\n",
    "spend = '07-21-2020'# Format for sp500 end date: mm-dd-yyy\n",
    "\n",
    "# Set the column name as the input of function 'get_index(thead, target, Date)' for quotes and sp500 based on the web tables\n",
    "q_setup = {'Date': 'Date', 'target': 'Close*'}\n",
    "sp_setup = {'Date': 'Date', 'target': 'Price'}\n",
    "\n",
    "# ETFs that no longer have data available\n",
    "speckey = {'BDCL': 'https://finance.yahoo.com/quote/BDCL/history?p=BDCL', \n",
    "           'FINU': 'https://finance.yahoo.com/quote/FINU/history?p=FINU', \n",
    "           'LBDC': 'https://finance.yahoo.com/quote/LBDC/history?p=LBDC',\n",
    "           'XLUY': 'https://finance.yahoo.com/quote/XLUY/history?p=XLUY',\n",
    "           'OILU': 'https://finance.yahoo.com/quote/OILD/history?p=OILD',\n",
    "           'GASX': 'https://finance.yahoo.com/quote/GASX/history?p=GASX',\n",
    "           'MIDZ': 'https://finance.yahoo.com/quote/MIDZ/history?p=MIDZ',\n",
    "           'RUSS': 'https://finance.yahoo.com/quote/RUSS/history?p=RUSS'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function\n",
    "### Quotes part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Sheet Quotes...\n"
     ]
    }
   ],
   "source": [
    "# Update Sheet Quotes\n",
    "print('Updating Sheet Quotes...')\n",
    "quotes_links = {} # stores links direct to historical data of quotes ETFs\n",
    "q_dic = {} # stores all data scrapped from web pages for quotes ETFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting FXR Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting IYJ Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting VIS Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting PPA Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting CGW Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting FIDU Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting XLP Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting XLY Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting IBUY Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting ITB Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting VCR Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting VDC Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting IYC Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting XRT Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting KXI Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting IYK Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting RHS Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting FSTA Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting FXG Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting PSL Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting PSCC Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting PBJ Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting JHMS Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting IECS Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting FDIS Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting ECON Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting FXD Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting RXI Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting CHIQ Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting PEJ Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting INCO Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting RCD Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting RTH Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting PSCD Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting PEZ Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting PBS Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting VXX Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting SH Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting SDOW Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting SPXU Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting SDS Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting DXD Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting QID Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting DOG Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting SBB Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting RWM Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting SQQQ Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting SPXS Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "MIDZ Data no longer available, skip MIDZ\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting RUSS Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n",
      "Extracting SMN Information from website...\n",
      "Locating target table on website...\n",
      "Scraping information from target table...\n"
     ]
    }
   ],
   "source": [
    "browser = browser_quotes(executable_path) # open firefox driver\n",
    "browser.get(yahoo) # open yahoo finance using firefox\n",
    "\n",
    "# iterate for all ETFs in sheet %Quotes\n",
    "for val in quotes.iter_cols(min_row = 2, min_col = 2, max_col = 164, max_row = 2, values_only = True):\n",
    "    key = val[0] # get the name of ETF\n",
    "    if key in speckey.keys():\n",
    "        # Check for the data availability\n",
    "        print('No new', key, 'data available. Extracting historical data for', key)\n",
    "        link = speckey[key]\n",
    "    else:\n",
    "        link = extract_quotes_hist_link(key)\n",
    "    \n",
    "    # Store links\n",
    "    if key not in quotes_links.keys():\n",
    "        quotes_links[key] = link\n",
    "    \n",
    "    # Extract data and store in dictionary\n",
    "    q_tbody, q_thead = locate_quotes_table(link, qstart, qend)\n",
    "    q_data_index, q_date_index = get_index(q_thead, q_setup['target'], q_setup['Date'])\n",
    "    q_data, q_date = interpret_table(q_tbody, q_data_index, q_date_index)\n",
    "    q_dic['Date'] = q_date\n",
    "    if key not in q_dic.keys():\n",
    "        q_dic[key] = q_data\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save links to a csv file. Run by demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all links to a csv. \n",
    "# If you want to save time in the future, change the code and use links directly loaded from this csv file.\n",
    "# If you do not want to load links from csv and would rather to obtain links through web scraping, then do not tun this block.\n",
    "with open('quotes_links.csv', 'w') as f:\n",
    "    for key in quotes_links.keys():\n",
    "        f.write(\"%s,%s\\n\"%(key,quotes_links[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save data frame to a csv file for safe. Run by demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot convert value in BDCL to float. Save as string.\n",
      "Cannot convert value in FINU to float. Save as string.\n",
      "Cannot convert value in LBDC to float. Save as string.\n",
      "Cannot convert value in XLUY to float. Save as string.\n",
      "Cannot convert value in GUSH to float. Save as string.\n",
      "Cannot convert value in OILU to float. Save as string.\n",
      "Cannot convert value in OILD to float. Save as string.\n",
      "Cannot convert value in GASX to float. Save as string.\n",
      "Cannot convert value in MIDZ to float. Save as string.\n",
      "Cannot convert value in RUSS to float. Save as string.\n"
     ]
    }
   ],
   "source": [
    "# Save all data for quotes to a csv file. Just in case. \n",
    "# If you do not want to save as a separate csv file, then do not run this block.\n",
    "q_df = construct_dataframe(q_dic)\n",
    "q_df.to_csv('quotes_' + qend +'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S&P500 Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Current google-chrome version is 84.0.4147\n",
      "[WDM] - Get LATEST driver version for 84.0.4147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Sheet S&P_500...\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Driver [/Users/zhouchunlei/.wdm/drivers/chromedriver/mac64/84.0.4147.30/chromedriver] found in cache\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: DeprecationWarning: use options instead of chrome_options\n"
     ]
    }
   ],
   "source": [
    "# Update Sheet SP500\n",
    "print('Updating Sheet S&P_500...')\n",
    "browser = adsblocker_Chrome(path_to_extension) # Open Google Chrome\n",
    "splinks = extract_sp_links(sp500) # Extract all links from sheet %S&P500\n",
    "browser.create_options() # Use adblocer for Chrome driver\n",
    "sp_dic = {} # Dictionary stores all data scrapped from web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting S&P 500 REAL ESTATE Information from website...\n",
      "Scraping information from target table...\n",
      "Extracting S&P 500 COMMUNICATION SERVICES Information from website...\n",
      "Scraping information from target table...\n",
      "Extracting S&P 500 HEALTH CARE Information from website...\n",
      "Scraping information from target table...\n",
      "Extracting S&P 500 FINANCIALS Information from website...\n",
      "Scraping information from target table...\n",
      "Extracting S&P 500 ENERGY Information from website...\n",
      "Scraping information from target table...\n",
      "Extracting S&P 500 UTILITIES Information from website...\n",
      "Scraping information from target table...\n",
      "Extracting S&P 500 INDUSTRIALS Information from website...\n",
      "Scraping information from target table...\n",
      "Extracting S&P 500 CONSUMER STAPLES Information from website...\n",
      "Scraping information from target table...\n",
      "Extracting S&P 500 INFORMATION TECHNOLOGY Information from website...\n",
      "Scraping information from target table...\n"
     ]
    }
   ],
   "source": [
    "# Iterate for ETFs from column L to T in sheet %S&P500\n",
    "for key,link in splinks.items():\n",
    "    # Extract data from web pages and store in dictionary\n",
    "    sp_tbody, sp_thead = locate_sp_table(key, link, spstart, spend)\n",
    "    sp_data_index, sp_date_index = get_index(sp_thead, sp_setup['target'], sp_setup['Date'])\n",
    "    sp_data, sp_date = interpret_table(sp_tbody, sp_data_index, sp_date_index)\n",
    "    sp_dic['Date'] = sp_date\n",
    "    if key not in sp_dic.keys():\n",
    "        sp_dic[key] = sp_data\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save links to a csv file. Run by demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all links to a csv. \n",
    "# If you want to change the code and use links directly loaded from this csv file, then run this block.\n",
    "# If you do not want to load links from csv and would rather to extract links from the worksheet, then do not tun this block.\n",
    "with open('sp500_links.csv', 'w') as f:\n",
    "    for key in splinks.keys():\n",
    "        f.write(\"%s,%s\\n\"%(key,splinks[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save data frame to a csv file for safe. Run by demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot convert value in S&P 500 HEALTH CARE to float. Save as string.\n",
      "Cannot convert value in S&P 500 INFORMATION TECHNOLOGY to float. Save as string.\n"
     ]
    }
   ],
   "source": [
    "# Save all data for quotes to a csv file. Just in case. \n",
    "# If you do not want to save as a separate csv file, then do not run this block.\n",
    "sp_df = construct_dataframe(sp_dic)\n",
    "sp_df.to_csv('sp500_' + spend + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process Finished! The updated file is available to review in your directory.\n"
     ]
    }
   ],
   "source": [
    "# Update sheet %Quotes\n",
    "update_quotes_sheet(quotes, q_df)\n",
    "\n",
    "# Update sheet %S&P500\n",
    "update_sp500_sheet(sp500, sp_df, spnrow, L = 12, A = 1)\n",
    "\n",
    "#TODO: Change the save path to your own directory before you run the code\n",
    "file.save('updated_' + spend + '.xlsx') #Save the updated file\n",
    "\n",
    "print('Process Finished! The updated file is available to review in your directory.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
